{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to build a social media sentiment analysis pipeline with scikit-learn\n",
    "\n",
    "*This is Part 4 of 5 in a series on building a sentiment analysis pipeline using scikit-learn. You can find Part 5 [here](./sentiment-pipeline-sklearn-5.ipynb), and the introduction [here](./sentiment-pipeline-sklearn-1.ipynb).*\n",
    "\n",
    "*Jump to:* \n",
    "\n",
    "* *[**Part 1 - Introduction and requirements**](./sentiment-pipeline-sklearn-1.ipynb)*\n",
    "* *[**Part 2 - Building a basic pipeline**](./sentiment-pipeline-sklearn-2.ipynb)*\n",
    "* *[**Part 3 - Adding a custom function to a pipeline**](./sentiment-pipeline-sklearn-3.ipynb)*\n",
    "* *[**Part 5 - Hyperparameter tuning in pipelines with GridSearchCV**](./sentiment-pipeline-sklearn-5.ipynb)*\n",
    "\n",
    "# Part 4 - Adding a custom feature to a pipeline with FeatureUnion\n",
    "\n",
    "Now that we know how to add a function to a pipeline, let's learn how to add the *output* of a function as an additional feature for our classifier.\n",
    "\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got 92 posts from page 1...\n",
      "got 88 posts from page 2...\n",
      "got 88 posts from page 3...\n",
      "got 91 posts from page 4...\n",
      "got 87 posts from page 5...\n",
      "got 89 posts from page 6...\n",
      "got 95 posts from page 7...\n",
      "got 93 posts from page 8...\n",
      "got 86 posts from page 9...\n",
      "got 90 posts from page 10...\n",
      "got all pages - 899 posts in total\n",
      "CPU times: user 710 ms, sys: 193 ms, total: 903 ms\n",
      "Wall time: 6.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from fetch_twitter_data import fetch_the_data\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = fetch_the_data()\n",
    "X, y = df.text, df.sentiment\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "tokenizer = nltk.casual.TweetTokenizer(preserve_case=False, reduce_len=True)\n",
    "count_vect = CountVectorizer(tokenizer=tokenizer.tokenize) \n",
    "classifier = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The function\n",
    "\n",
    "For this example, we'll append the length of the post to the output of the count vectorizer, the thinking being that longer posts could be more likely to be polarized (such as someone going on a rant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tweet_length(text):\n",
    "    return len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding new features\n",
    "scikit-learn has a nice [FeatureUnion class](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html) that enables you to essentially concatenate more feature columns to the output of the count vectorizer. This is useful for adding \"meta\" features.\n",
    "\n",
    "It's pretty silly, but to add a feature in a `FeatureUnion`, it has to come back as a numpy array of `dim(rows, num_cols)`. For our purposes in this example, we're only bringing back a single column, so we have to reshape the output to `dim(rows, 1)`. Gotta love it. So first, we'll define a method to reshape the output of a function into something acceptable for `FeatureUnion`. After that, we'll build our function that will wrap a function to be easily pipeline-able."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def reshape_a_feature_column(series):\n",
    "    return np.reshape(np.asarray(series), (len(series), 1))\n",
    "\n",
    "def pipelinize_feature(function, active=True):\n",
    "    def list_comprehend_a_function(list_or_series, active=True):\n",
    "        if active:\n",
    "            processed = [function(i) for i in list_or_series]\n",
    "            processed = reshape_a_feature_column(processed)\n",
    "            return processed\n",
    "#         This is incredibly stupid and hacky, but we need it to do a grid search with activation/deactivation.\n",
    "#         If a feature is deactivated, we're going to just return a column of zeroes.\n",
    "#         Zeroes shouldn't affect the regression, but other values may.\n",
    "#         If you really want brownie points, consider pulling out that feature column later in the pipeline.\n",
    "        else:\n",
    "            return reshape_a_feature_column(np.zeros(len(list_or_series)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding the function and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null accuracy: 38.67%\n",
      "accuracy score: 62.22%\n",
      "model is 23.56% more accurate than null accuracy\n",
      "---------------------------------------------------------------------------\n",
      "Confusion Matrix\n",
      "\n",
      "Predicted  negative  neutral  positive  __all__\n",
      "Actual                                         \n",
      "negative         28       19        17       64\n",
      "neutral          10       44        20       74\n",
      "positive          5       14        68       87\n",
      "__all__          43       77       105      225\n",
      "---------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "                precision    recall  F1_score support\n",
      "Classes                                              \n",
      "negative         0.651163    0.4375  0.523364      64\n",
      "neutral          0.571429  0.594595  0.582781      74\n",
      "positive         0.647619  0.781609  0.708333      87\n",
      "__avg / total__  0.623569  0.622222  0.614427     225\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn_helpers import pipelinize, genericize_mentions, train_test_and_evaluate\n",
    "\n",
    "\n",
    "sentiment_pipeline = Pipeline([\n",
    "        ('genericize_mentions', pipelinize(genericize_mentions, active=True)),\n",
    "        ('features', FeatureUnion([\n",
    "                    ('vectorizer', count_vect),\n",
    "                    ('post_length', pipelinize_feature(get_tweet_length, active=True))\n",
    "                ])),\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "\n",
    "sentiment_pipeline, confusion_matrix = train_test_and_evaluate(sentiment_pipeline, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Almost done\n",
    "Even though we did it in kind of a weird way, we are now able to add arbitrary functions as new feature columns!\n",
    "\n",
    "![](http://gifrific.com/wp-content/uploads/2012/09/Are-We-Having-Fun-Yet-Party-Down.gif \"Awwwww yeah\")\n",
    "\n",
    "We're now ready for the last part of the series - doing a parameter grid search on the pipeline. [Come on, let's do it!](./sentiment-pipeline-sklearn-5.ipynb)\n",
    "\n",
    "*This is Part 4 of 5 in a series on building a sentiment analysis pipeline using scikit-learn. You can find Part 5 [here](./sentiment-pipeline-sklearn-5.ipynb), and the introduction [here](./sentiment-pipeline-sklearn-1.ipynb).*\n",
    "\n",
    "*Jump to:* \n",
    "\n",
    "* *[**Part 1 - Introduction and requirements**](./sentiment-pipeline-sklearn-1.ipynb)*\n",
    "* *[**Part 2 - Building a basic pipeline**](./sentiment-pipeline-sklearn-2.ipynb)*\n",
    "* *[**Part 3 - Adding a custom function to a pipeline**](./sentiment-pipeline-sklearn-3.ipynb)*\n",
    "* *[**Part 5 - Hyperparameter tuning in pipelines with GridSearchCV**](./sentiment-pipeline-sklearn-5.ipynb)*"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/bfeb07535373d908f1bba6842e7797d9"
  },
  "gist": {
   "data": {
    "description": "Sentiment Blog Rough Draft",
    "public": false
   },
   "id": "bfeb07535373d908f1bba6842e7797d9"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
